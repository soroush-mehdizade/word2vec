# Word2Vec from Scratch

This repository contains a Jupyter Notebook implementation of the **Word2Vec** algorithm, a fundamental technique in **Natural Language Processing (NLP)** used to represent words as dense numerical vectors.

The notebook explains the core ideas behind Word2Vec and demonstrates how semantic relationships between words can be learned from text data. By mapping words into a continuous vector space, the model captures contextual and semantic similarities between words.

## Overview

Word2Vec is a word embedding technique that transforms words into vectors such that words appearing in similar contexts have similar representations. These embeddings are widely used in NLP tasks such as text classification, clustering, information retrieval, and language modeling.

This project focuses on understanding the algorithmic intuition and practical implementation rather than using high-level black-box libraries.

## Features

- Text preprocessing and corpus preparation  
- Implementation of the Word2Vec algorithm  
- Training word embeddings from text data  
- Exploring word similarities using vector representations  
- Educational, step-by-step notebook format  

## Technologies Used

- Python  
- NumPy  
- Jupyter Notebook
- nltk
- gensim

## Use Cases

- Learning word embeddings and distributional semantics  
- Educational reference for NLP fundamentals  
- Experimentation with vector-based word representations  

## Getting Started

1. Clone the repository:
   ```bash
   git clone https://github.com/soroush-mehdizade/word2vec.git
